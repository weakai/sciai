---
title: huggingface transformers
---

- [Huggingface Transformeræ•™ç¨‹(ä¸€)](http://fancyerii.github.io/2021/05/11/huggingface-transformers-1/)

```python
from transformers import AutoTokenizer, AutoModelForSequenceClassification

model_name = "nlptown/bert-base-multilingual-uncased-sentiment"
model_name = "distilbert-base-uncased-finetuned-sst-2-english"

model = AutoModelForSequenceClassification.from_pretrained(model_name)
tokenizer = AutoTokenizer.from_pretrained(model_name)
classifier = pipeline('sentiment-analysis', model=model, tokenizer=tokenizer)

inputs = tokenizer("We are very happy to show you the ğŸ¤— Transformers library.")

pt_batch = tokenizer(
    ["We are very happy to show you the ğŸ¤— Transformers library.", "We hope you don't hate it."],
    padding=True,
    truncation=True,
    max_length=512,
    return_tensors="pt"
)

# æŸ¥çœ‹æ ‡è®°å¤„ç†åçš„æ‰¹æ•°æ®
for key, value in pt_batch.items():
    print(f"{key}: {value.numpy().tolist()}")

# ä½¿ç”¨ï¼Œé€å…¥æ¨¡å‹
pt_outputs = model(**pt_batch)
print(pt_outputs)
# SequenceClassifierOutput(loss=None, logits=tensor([[-4.0833,  4.3364],
#     [ 0.0818, -0.0418]], grad_fn=<AddmmBackward>), hidden_states=None, attentions=None)
# all outputs are objects that contain the modelâ€™s final activations along with other metadata

# æ¿€æ´»å‡½æ•° softmax
# å°† logits è½¬åŒ–ä¸ºé¢„æµ‹æ•°å€¼
from torch import nn
pt_predictions = nn.functional.softmax(pt_outputs.logits, dim=-1)
print(pt_predictions) # Output: tensor
```

å¦‚æœä½ è¾“å…¥äº† label åˆ°æ¨¡å‹ä¸­ï¼Œæ¨¡å‹çš„è¾“å‡ºå¯¹è±¡ä¹Ÿä¼šåŒ…å« loss å±æ€§

```python
import torch
pt_outputs = pt_model(**pt_batch, labels = torch.tensor([1, 0]))
print(pt_outputs) # Outputs: SequenceClassifierOutput
```

ä¿å­˜ä»¥åŠåŠ è½½é¢„è®­ç»ƒæ•°æ®

```python
tokenizer.save_pretrained(save_directory)
model.save_pretrained(save_directory)

from transformers import AutoModel
tokenizer = AutoTokenizer.from_pretrained(save_directory)
model = AutoModel.from_pretrained(save_directory, from_tf=True)
```

ä»æ¨¡å‹ä¸­æå– éšè—çŠ¶æ€ å’Œæƒé‡

```python
pt_outputs = pt_model(**pt_batch, output_hidden_states=True, output_attentions=True)
all_hidden_states  = pt_outputs.hidden_states
all_attentions = pt_outputs.attentions
```

ä»æºç è€Œä¸æ˜¯ç”¨ AutoModel åŠ è½½æ•°æ®å’Œæ¨¡å‹

```python
from transformers import DistilBertTokenizer, DistilBertForSequenceClassification
model_name = "distilbert-base-uncased-finetuned-sst-2-english"
model = DistilBertForSequenceClassification.from_pretrained(model_name)
tokenizer = DistilBertTokenizer.from_pretrained(model_name)
```

å®šåˆ¶æ¨¡å‹

```python
from transformers import DistilBertConfig, DistilBertTokenizer, DistilBertForSequenceClassification
# ç”Ÿæˆé…ç½®
config = DistilBertConfig(n_heads=8, dim=512, hidden_dim=4*512)
model = DistilBertForSequenceClassification(config)

tokenizer = DistilBertTokenizer.from_pretrained('distilbert-base-uncased')
```

ä½ å¦‚æœåªæ”¹å˜ head çš„ label æ•°ï¼Œä½ æ˜¯ä¸éœ€è¦æ”¹å˜ model çš„ pretrained body çš„ã€‚è¿™ç§æ–¹å¼çš„æ¨¡å‹å®šåˆ¶ä¼šæ¯”è¾ƒç®€å•

```python
from transformers import DistilBertConfig, DistilBertTokenizer, DistilBertForSequenceClassification
model_name = "distilbert-base-uncased"
model = DistilBertForSequenceClassification.from_pretrained(model_name, num_labels=10)

tokenizer = DistilBertTokenizer.from_pretrained(model_name)

```

## å“²å­¦

# Glossary

- CLM: causal language modeling, a pretraining task where the model reads the texts in order and has to predict the next word. Itâ€™s usually done by reading the whole sentence but using a mask inside the model to hide the future tokens at a certain timestep.

- MLM: masked language modeling, a pretraining task where the model sees a corrupted version of the texts, usually done by masking some tokens randomly, and has to predict the original text.

- multimodal: a task that combines texts with another kind of inputs (for instance images).

- 

- NLG: natural language generation, all tasks related to generating text (for instance talk with transformers, translation).

- NLU: natural language understanding, all tasks related to understanding what is in a text (for instance classifying the whole text, individual words).

## Input IDs

æ¨¡å‹è¾“å…¥çš„å”¯ä¸€å¿…é¡»å‚æ•°

```python
from transformers import BertTokenizer
tokenizer = BertTokenizer.from_pretrained("bert-base-cased")
sequence = "A Titan RTX has 24GB of VRAM"

# è½¬åŒ–ä¸ºå¯è¯»å½¢å¼
tokenized_sequence = tokenizer.tokenize(sequence)
print(tokenized_sequence)

# è½¬åŒ–ä¸º Input IDs
inputs = tokenizer(sequence)
```



## Good words

wait

These objects are described in greater detail here.

activations æ¿€æ´»ï¼ˆå‡½æ•°ï¼‰

 browse the source code

 the auto magic

Transformers is an opinionated library built for ä¸“ä¸ºè°è®¾è®¡

hands-on practitioners äº²èº«å®è·µè€…

Be as easy and fast to use as possible

Every model is different yet bears similarities with the others. 

which are detailed here alongside usage examples

## å‚è€ƒ

https://huggingface.co/transformers/quicktour.html